{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textatistic as tx\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "\n",
    "from threading import Thread\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   DATA CLEANING\n",
    "\n",
    "neg_data = []\n",
    "pos_data = []\n",
    "\n",
    "dir_neg = r\"/home/kinged/Documents/Random/NLP_init/test/neg\" #\"C:\\Users\\KingED\\Documents\\Random\\NLP_init\\test\\neg\" for windows\n",
    "dir_pos = r\"/home/kinged/Documents/Random/NLP_init/test/neg\" #\"C:\\Users\\KingED\\Documents\\Random\\NLP_init\\test\\pos\" for windows\n",
    "\n",
    "def maker(loc: list, path: str):\n",
    "    for files in os.listdir(path):\n",
    "        os.chdir(path)\n",
    "        with open(files, 'rb+') as file:\n",
    "            loc.append(file.readlines())\n",
    "       \n",
    "maker(neg_data, dir_neg)\n",
    "maker(pos_data, dir_pos)\n",
    "\n",
    "\n",
    "pattern_1 = r\"^b\\\"|^b\\'\"\n",
    "pattern_2 = r\"<br\\s\\/\\>\"\n",
    "patter_3 = r'\\\\|\\\"|\\''\n",
    "\n",
    "pos_word = []\n",
    "neg_word = []\n",
    "\n",
    "#FIXME : convert these loops into functions\n",
    "for c in pos_data:\n",
    "    word = str(c[0])\n",
    "    word = re.sub(pattern_2, \"\", word)\n",
    "    word = re.sub(pattern_1, \"\", word)\n",
    "    word = re.sub(patter_3, \"\", word)\n",
    "    pos_word.append(word)\n",
    "    \n",
    "for c in neg_data:\n",
    "    word = str(c[0])\n",
    "    word = re.sub(pattern_2, \"\", word)\n",
    "    word = re.sub(pattern_1, \"\", word)\n",
    "    word = re.sub(patter_3, \"\", word)\n",
    "    neg_word.append(word)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: make this function compute the gunningfog and flesch scores at the same time \n",
    "\n",
    "fog_scores = []\n",
    "flesch_score = []\n",
    "issues = []\n",
    "for x in pos_word:\n",
    "    try:\n",
    "        fog = tx.gunningfog_score(x)\n",
    "        flesch = tx.flesch_score(x)\n",
    "        fog_scores.append(fog)\n",
    "        flesch_score.append(flesch)\n",
    "      \n",
    "    except ZeroDivisionError:\n",
    "        issues.append(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average flesch score for poistive reviews is 80.61120107848635\n",
      "the average runningfog scores for the postive reviews is 9.199174109143955\n"
     ]
    }
   ],
   "source": [
    "average_fog = sum(fog_scores)/ len(fog_scores)\n",
    "average_flesch = sum(flesch_score)/ len(flesch_score)\n",
    "\n",
    "print(f\"the average flesch score for poistive reviews is {average_flesch}\")\n",
    "print(f\"the average runningfog scores for the postive reviews is {average_fog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the usual disclaimer - I do not give 1 star ra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Tender Hook, or, Who Killed The Australian...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go Fish garnered Rose Troche rightly or wrongl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are movies that are leaders, and movies ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do the following: Get a copy of this movie and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>I like CKY and Viva La Bam, so I couldnt resis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I love the Jurassic Park movies, they are thre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>The movie is not as funny as the directors pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>I watched the version with pathetic American o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>this is an example of a movie that can have gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  score\n",
       "0      the usual disclaimer - I do not give 1 star ra...      0\n",
       "1      The Tender Hook, or, Who Killed The Australian...      0\n",
       "2      Go Fish garnered Rose Troche rightly or wrongl...      0\n",
       "3      There are movies that are leaders, and movies ...      0\n",
       "4      Do the following: Get a copy of this movie and...      0\n",
       "...                                                  ...    ...\n",
       "24995  I like CKY and Viva La Bam, so I couldnt resis...      1\n",
       "24996  I love the Jurassic Park movies, they are thre...      1\n",
       "24997  The movie is not as funny as the directors pre...      1\n",
       "24998  I watched the version with pathetic American o...      1\n",
       "24999  this is an example of a movie that can have gr...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_pos = pd.DataFrame({'review': pos_word, 'score': 1})\n",
    "doc_neg = pd.DataFrame({'review': neg_word, 'score': 0})\n",
    "\n",
    "data  = pd.concat([doc_neg, doc_pos], ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file aready exists\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r\"/home/kinged/Documents/Random/NLP_init\")\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file != 'review.csv':\n",
    "        data.to_csv('review.csv')\n",
    "    else:\n",
    "        print(\"file aready exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the usual disclaimer - I do not give 1 star ratings to movies which are harmless, bad, low budget and silly, although they may deserve it. These films are often funny, and get rated 2-4 based sheerly on entertainment value - not as a representation of their exemplary film artistry. This film fits this model perfectly. It is a Mexican monster movie, riddled with voice-over narrative and extremely weak not-so-special effects. The makeup is not that bad, and the acting is sometimes quite entertaining, but this film is almost as silly as Aliens vs Predator and the script isnt half as slick (Aliens vs Predator might get a 1 from me, but I want to see it again before I commit).The plot is ridiculous, but deliciously convoluted. If youve read this far, you must really want to know... A group of remarkably unscientific scientists comprise the main characters. Most of them are heroes - sort of - but one is (of course) mad, and quite perverse. This mad scientist invents a laughable nuclear powered robot (who looks a bit like the tin man from Wizard of Oz, but has a human face inexplicably located inside its head). An Aztec mummy, discovered by the same scientist whose wife just so happens to have been an Aztec princess in a past life (dont ask), is pitted against the robot for the big climax the fight scene alone is enough to put the most stoic movie watcher on the floor in belly laughs.For what its worth, given the budget and the utter silliness of the script, this is a very entertaining low budget goof ball monster movie. If youre into that sort of thing, go for it.',\n",
       "       0], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array = data.values\n",
    "data_array[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/kinged/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         words\u001b[38;5;241m.\u001b[39mappend(word_tokenize(word[x]))\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\n\u001b[0;32m---> 14\u001b[0m pos_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtoken_maker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m neg_tokens \u001b[38;5;241m=\u001b[39m token_maker(neg_word)\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mtoken_maker\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      9\u001b[0m words \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pos_word)):\n\u001b[0;32m---> 11\u001b[0m     words\u001b[38;5;241m.\u001b[39mappend(\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m words\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/kinged/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: TOKENIZATION\n",
    "TODO: REMOVE STOPWORDS\n",
    "TODO: LEMMATIZE\n",
    "TODO: WORD AND CHARACTER COUNT\n",
    "\n",
    "\"\"\"\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def token_maker(word: list) -> list:\n",
    "    words = []\n",
    "    for x in range(len(pos_word)):\n",
    "        words.append(word_tokenize(word[x]))\n",
    "    return words\n",
    "\n",
    "pos_tokens = token_maker(pos_word)\n",
    "neg_tokens = token_maker(neg_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACEY\n",
    "\"\"\"\n",
    "TODO: NER\n",
    "TODO: POS TAGGING\n",
    "\"\"\"\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(doc)\n",
    "\n",
    "# def ner_pos():\n",
    "#     ners = [(token.text, token.label_) for word in doc]\n",
    "#     tags = [(token.text, token.tags_) for word in doc]\n",
    "    \n",
    "word = word_tokenize(pos_word[1])\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "tfid = TfidfVectorizer()\n",
    "\n",
    "# lemma.lemmatize(word)\n",
    "tfidword = tfid.fit_transform(data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KingED\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 'nt', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 68662)\n",
      "the accuracy of the model is: 87.14666666666666\n"
     ]
    }
   ],
   "source": [
    "counter = CountVectorizer(lowercase=True,\n",
    "                          strip_accents='ascii',\n",
    "                          tokenizer=None,\n",
    "                          stop_words=list(STOP_WORDS),\n",
    "                          )\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "data['score'] = encoder.fit_transform(data['score'])\n",
    "\n",
    "x = data['review'].values\n",
    "y = data['score'].values\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, shuffle=True, test_size=0.3, random_state=34)\n",
    "\n",
    "\n",
    "new_xtrain = counter.fit_transform(xtrain)\n",
    "new_xtest = counter.transform(xtest)\n",
    "\n",
    "print(new_xtrain.shape)\n",
    "\n",
    "model = MultinomialNB(alpha=1)\n",
    "model.fit(new_xtrain, ytrain)\n",
    "pred = model.predict(new_xtest)\n",
    "print(f\"the accuracy of the model is: {accuracy_score(pred, ytest) * 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
